{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdda34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, LSTM, Bidirectional, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a6d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/NLData1.csv')\n",
    "df = df.drop(columns=['Time', 'Speaker', 'ID'])\n",
    "df = df.drop(labels=0)\n",
    "df.drop(df[pd.isna(df.Text)].index, inplace=True) # Drop the empty rows\n",
    "df = df.reset_index(drop=True)\n",
    "# df_train = df[:450]\n",
    "# df_test = df[450:500] # Here I didn't use the full data instead just doing validations\n",
    "# df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6c46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetItAllDone:\n",
    "    \n",
    "    # This class plays the role of decorator.\n",
    "    # No need much attention.\n",
    "    class clean_text:   \n",
    "        def __init__(self, decorated):\n",
    "            self.decorated = decorated\n",
    "            \n",
    "        def clean(self, sentence):\n",
    "            '''Clean all irrelavent tokens in the input sentence'''\n",
    "            nlp = spacy.load(\"ja_core_news_sm\")\n",
    "            delete_pos = [\"PUNCT\", \"SPACE\", \"SYM\", \"NUM\"]\n",
    "            doc = nlp(sentence)\n",
    "            word_list = [str(token) for token in doc if token.pos_ not in delete_pos]\n",
    "            return ' '.join(word_list)\n",
    "      \n",
    "        def __call__(self, *args, **kwargs):\n",
    "            df = self.decorated(*args, **kwargs)[0]\n",
    "            num_train = self.decorated(*args, **kwargs)[1]\n",
    "            num_test = self.decorated(*args, **kwargs)[2]\n",
    "            df['Text'] = df['Text'].apply(self.clean)\n",
    "            df.drop_duplicates(subset='Text', keep=False, inplace=True)\n",
    "            df = df.reset_index(drop=True)\n",
    "            df_train = df[:num_train]\n",
    "            df_test = df[num_train:num_train+num_test]\n",
    "            df_test = df_test.reset_index(drop=True)\n",
    "            return (df_train, df_test)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ja_embedding = WordEmbeddings('ja-crawl')\n",
    "        self.ja_forward_embedding = FlairEmbeddings('ja-forward')\n",
    "        self.ja_backward_embedding = FlairEmbeddings('ja-backward')\n",
    "\n",
    "        self.stacked_embedding = StackedEmbeddings([\n",
    "            self.ja_embedding,\n",
    "            self.ja_forward_embedding,\n",
    "            self.ja_backward_embedding\n",
    "        ])\n",
    "    \n",
    "    # This inner method will prepare the data in the form ready for training.\n",
    "    # We will be using this method to get training and test dataframe ready.\n",
    "    @clean_text\n",
    "    def _prepare_data(dataframe, column_to_prepare, num_train, num_test):\n",
    "        '''\n",
    "        Inputs:\n",
    "            dataframe: a dataframe to get prepared\n",
    "            column_to_prepare: the colunmn to get one-hot encoded\n",
    "            num_train: the desired number of training data\n",
    "            num_test: the desired number of testing data\n",
    "        Returns:\n",
    "            a tuple with training and testing dataframe\n",
    "        '''\n",
    "        num_train = num_train\n",
    "        num_test = num_test\n",
    "        data = dataframe\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        property_list = list(data.columns)\n",
    "        property_list.remove('Text')\n",
    "        df_modified = data.drop(columns=property_list)\n",
    "        df_to_return = pd.concat([df_modified, data[column_to_prepare]], axis=1)\n",
    "        df_to_return['Code_Modified'] = df_to_return[column_to_prepare]\n",
    "        df_to_return['Code_Modified'] = df_to_return['Code_Modified'].replace(np.nan, 'none', regex=True)\n",
    "        df_to_return['Code_Modified'] = le.fit_transform(df_to_return['Code_Modified'])\n",
    "        #df_train = df_to_return[:num_train]\n",
    "        #df_test = df_to_return[num_train:num_train+num_test]\n",
    "        return (df_to_return, num_train, num_test)\n",
    "    \n",
    "    # This method will generate the training data\n",
    "    def generate_train_data(self, data_to_train, column_to_prepare, num_train, num_test, batch_size, max_length, num_classes, emb_size):\n",
    "        '''\n",
    "        Inputs:\n",
    "            data_to_train: the dataframe of training data\n",
    "            column_to_prepare: the column to get one-hot encoded\n",
    "            num_train: the desired number of training data\n",
    "            num_test: the desired number of testing data\n",
    "            batch_size: number of training examples running through your network in one batch\n",
    "            max_length: the number of tokens to take in one sentence\n",
    "            num_classes: the number of classes in the target\n",
    "            emb_size: the dimension of embedding space, default to be 4396\n",
    "        Returns:\n",
    "            numpy arrays of batch of training data ready to be fed into the model\n",
    "        '''\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        while True:\n",
    "            data = self._prepare_data(data_to_train, column_to_prepare, num_train, num_test)[0]\n",
    "            data = data.sample(frac=1)\n",
    "            for _, row in data.iterrows():\n",
    "                my_sent = row['Text']\n",
    "                sentence = Sentence(my_sent)\n",
    "                self.stacked_embedding.embed(sentence)\n",
    "                x = []\n",
    "                for token in sentence:\n",
    "                    x.append(token.embedding.cpu().detach().numpy())\n",
    "                    if len(x) == max_length:\n",
    "                        break\n",
    "                \n",
    "                while len(x) < max_length:\n",
    "                    x.append(np.zeros(emb_size))\n",
    "                \n",
    "                y = np.zeros(num_classes)\n",
    "                y[row[\"Code_Modified\"]] = 1\n",
    "                \n",
    "                x_batch.append(x)\n",
    "                y_batch.append(y)\n",
    "                \n",
    "                if len(y_batch) == batch_size:\n",
    "                    yield np.array(x_batch), np.array(y_batch)\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "    \n",
    "    # This method will generate dataframe for testing\n",
    "    def generate_test_data(self, data_to_test, column_to_prepare, num_train, num_test, batch_size, max_length, emb_size):\n",
    "        '''\n",
    "        Inputs:\n",
    "            data_to_test: the dataframe of testing data\n",
    "            column_to_prepare: the column to get one-hot encoded\n",
    "            num_train: the desired number of training data\n",
    "            num_test: the desired number of testing data\n",
    "            batch_size: number of training examples running through your network in one batch\n",
    "            max_length: the number of tokens to take in one sentence\n",
    "            emb_size: the dimension of embedding space, default to be 4396\n",
    "        Returns:\n",
    "            numpy arrays of batch of testing data ready to be used\n",
    "        '''\n",
    "        x_batch = []\n",
    "        data = self._prepare_data(data_to_test, column_to_prepare, num_train, num_test)[1]\n",
    "        while True:\n",
    "            for _, row in data.iterrows():\n",
    "                my_sent = row['Text']\n",
    "                sentence = Sentence(my_sent)\n",
    "                self.stacked_embedding.embed(sentence)\n",
    "                x = []\n",
    "                for token in sentence:\n",
    "                    x.append(token.embedding.cpu().detach().numpy())\n",
    "                    if len(x) == max_length:\n",
    "                        break\n",
    "\n",
    "                while len(x) < max_length:\n",
    "                    x.append(np.zeros(emb_size))\n",
    "\n",
    "                x_batch.append(x)            \n",
    "                if len(x_batch) == batch_size:\n",
    "                    yield np.array(x_batch)\n",
    "                    x_batch = []\n",
    "                    \n",
    "    # Declare the model\n",
    "    def declare_model(self, batch_size, max_len, emb_size, gru_size, num_classes):\n",
    "        '''\n",
    "        Inputs:\n",
    "            gru_size: positive integer, dimensionality of the output space\n",
    "        Returns:\n",
    "            a model\n",
    "        '''\n",
    "        sample = Input(batch_shape=(batch_size, max_len, emb_size))\n",
    "        gru_out = Bidirectional(GRU(gru_size, return_sequences=True))(sample)\n",
    "        #lstm_out = Bidirectional(LSTM(gru_size, return_sequences=True))(sample)\n",
    "        gru_out = Flatten()(gru_out)\n",
    "        #lstm_out = Flatten()(lstm_out)\n",
    "        predictions = Dense(num_classes, activation='sigmoid')(gru_out)\n",
    "\n",
    "        model = Model(inputs=sample, outputs=[predictions])\n",
    "        model.compile(optimizer=Adam(),loss='categorical_crossentropy', metrics=[\"acc\"])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    # Fetch the targets of testing data.\n",
    "    # We will be using this method to make comparison with predicted values\n",
    "    def fetch_target(self, data_to_test, column_to_prepare, num_train, num_test):\n",
    "        data_to_fetch = self._prepare_data(data_to_test, column_to_prepare, num_train, num_test)[1]\n",
    "        ans = [data_to_fetch['Code_Modified'][i] for i in range(len(data_to_fetch))]\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe888e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = GetItAllDone().generate_train_data(df, column_to_prepare='Relating Levels', num_train=300, num_test=100, batch_size=10, max_length=10, num_classes=6, emb_size=4396)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf0706b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(10, 10, 4396)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (10, 10, 40)              530160    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (10, 400)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (10, 6)                   2406      \n",
      "=================================================================\n",
      "Total params: 532,566\n",
      "Trainable params: 532,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = GetItAllDone().declare_model(batch_size=10, max_len=10, emb_size=4396, gru_size=20, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b007b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 30 steps\n",
      "Epoch 1/5\n",
      "28/30 [===========================>..] - ETA: 12s - loss: 1.0275 - acc: 0.83212021-08-15 16:05:02,018 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "30/30 [==============================] - 181s 6s/step - loss: 0.9931 - acc: 0.8400\n",
      "Epoch 2/5\n",
      "22/30 [=====================>........] - ETA: 1:08 - loss: 0.5750 - acc: 0.87732021-08-15 16:08:27,633 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "30/30 [==============================] - 238s 8s/step - loss: 0.6003 - acc: 0.8667\n",
      "Epoch 3/5\n",
      "23/30 [======================>.......] - ETA: 1:01 - loss: 0.4113 - acc: 0.86092021-08-15 16:12:35,943 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "30/30 [==============================] - 237s 8s/step - loss: 0.4107 - acc: 0.8667\n",
      "Epoch 4/5\n",
      "11/30 [==========>...................] - ETA: 3:31 - loss: 0.2388 - acc: 0.91822021-08-15 16:15:11,788 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "30/30 [==============================] - 237s 8s/step - loss: 0.2886 - acc: 0.8667\n",
      "Epoch 5/5\n",
      "27/30 [==========================>...] - ETA: 24s - loss: 0.2091 - acc: 0.87042021-08-15 16:20:43,644 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "30/30 [==============================] - 239s 8s/step - loss: 0.2147 - acc: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff95b086190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(gen, steps_per_epoch=30, epochs=5, max_queue_size=15, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68bbba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_test = GetItAllDone().generate_test_data(df, column_to_prepare='Relating Levels', num_train=300, num_test=100, batch_size=10, max_length=10, emb_size=4396)\n",
    "predict = np.argmax(model.predict(gen_test, steps=10), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa465443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans = GetItAllDone().fetch_target(df, column_to_prepare='Relating Levels', num_train=300, num_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77035ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "acc = sum([1 for a, b in zip(predict, ans) if a==b])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb52d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109cdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
